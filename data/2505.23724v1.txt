arXiv:2505.23724v1  [cs.LG]  29 May 2025SC-LoRA: Balancing Efficient Fine-tuning and Knowledge Preservation via
Subspace-Constrained LoRA
Minrui Luo∗1,2, Fuhang Kuang∗1, Yu Wang3, Zirui Liu1, Tianxing He†1,2
1Institute for Interdisciplinary Information Sciences, Tsinghua University
2Shanghai Qi Zhi Institute
3Institute of Information Engineering, Chinese Academy of Sciences
{luomr22,kfh22,liu-zr22}@mails.tsinghua.edu.cn;
wangyu2002@iie.ac.cn hetianxing@mail.tsinghua.edu.cn
Abstract
Parameter-Efficient Fine-Tuning (PEFT)
methods, particularly Low-Rank Adaptation
(LoRA), are indispensable for efficiently
customizing Large Language Models (LLMs).
However, vanilla LoRA suffers from slow
convergence speed and knowledge forgetting
problems. Recent studies have leveraged
the power of designed LoRA initialization,
to enhance the fine-tuning efficiency, or to
preserve knowledge in the pre-trained LLM.
However, none of these works can address
the two cases at the same time. To this end,
we introduce Subspace- Constrained LoRA
(SC-LoRA ), a novel LoRA initialization
framework engineered to navigate the trade-off
between efficient fine-tuning and knowledge
preservation. We achieve this by constraining
the output of trainable LoRA adapters in a
low-rank subspace, where the context infor-
mation of fine-tuning data is most preserved
while the context information of preserved
knowledge is least retained, in a balanced way.
Such constraint enables the trainable weights
to primarily focus on the main features of
fine-tuning data while avoiding damaging the
preserved knowledge features. We provide
theoretical analysis on our method, and
conduct extensive experiments including
safety preservation and world knowledge
preservation , on various downstream tasks.
In our experiments, SC-LoRA succeeds in
delivering superior fine-tuning performance
while markedly diminishing knowledge
forgetting, surpassing contemporary LoRA
initialization methods.
1 Introduction
Fine-tuning effectively adapts large language mod-
els to downstream tasks (Luo et al., 2025; Yu et al.,
2024). Due to the high computational cost of full
fine-tuning, parameter-efficient fine-tuning (PEFT)
∗Equal contribution.
†Corresponding author.methods (Xu et al., 2023; Han et al., 2024) have
been proposed to reduce the number of trainable
parameters while maintaining good fine-tuning per-
formance. Among various PEFT methods, LoRA
(Hu et al., 2022) is a simple yet efficient approach
that introduces trainable low-rank adaptation mod-
ules for tuning. While LoRA offers significant
parameter efficiency, it has two important prob-
lems: (1) the convergence speed of the fine-tuning
process is relatively slow due to the noise and zero
initialization of adapter modules; (2) it potentially
leads to catastrophic forgetting problem (Goodfel-
low et al., 2015) as other fine-tuning methods do,
such as harming the world knowledge stored in pre-
trained LLMs (Yang et al., 2024), and degrading
the safety of aligned LLMs (Qi et al., 2024).
Recent works have found that carefully designed
initialization on LoRA adapters can solve these
problems. (Meng et al., 2024) initializes LoRA
adapters by parts of Singular Value Decomposi-
tion (SVD) of original weight W0, leading to faster
convergence and improved performance by encap-
sulating the most significant information stored
inW0. Later works (Yang et al., 2024; Paischer
et al., 2024) initialize LoRA weights based on
semantic information stored in the activations of
each layer on the target fine-tuning dataset. These
data-driven approaches successfully enhance the
fine-tuning speed and performance. Towards catas-
trophic forgetting problem in LoRA fine-tuning,
(Yang et al., 2024) proposes to initialize LoRA
weights by the least principal directions of world
knowledge data features, successfully alleviating
the forgetting problem. However, these works can
only solve either side of the two problems, but
do not consider the trade-off between enhancing
fine-tuning performance and preserving pre-trained
knowledge, which is a common need when doing
parameter-efficient fine-tuning.
In this paper, we introduce Subspace-
Constrained LoRA , a balanced LoRA scheme that
1achieves both better fine-tuning results and good
preservation of knowledge in LLMs. Specifically,
we compute directions of linear layer output that
align with the principal directions of fine-tuning
data and at the same time are orthogonal to the
principal directions of preserved knowledge. These
directions are then used to initialize the adapter
weights, constraining the output vectors (of each
adapter layer) in a subspace spanned by these
directions. This constraint intuitively makes the
updating terms to focus on the fine-tuning data
information, while avoiding affecting the preserved
knowledge. By extensive experiments, we verify
that by such constraint on balanced directions,
our method achieves both efficient fine-tuning
and excellent knowledge preservation, solving the
problems that previous methods cannot address. In
conclusion, our contribution includes:
1.We propose SC-LoRA, a balanced LoRA
scheme that can achieve efficient fine-tuning
and knowledge preservation at the same time,
which previous methods cannot handle.
2.We provide theoretical proofs to explain our
strategies, including analysis on subspace se-
lection and initialization setting.
3.We conduct extensive experiments regarding
both safety preservation andworld knowledge
preservation on various downstream tasks,
verifying the effectiveness of our method.
2 Related Work
Parameter-Efficient Fine-Tuning (PEFT).
Modern large language models (LLMs) with bil-
lions of parameters face significant computational
and memory challenges during full-parameter fine-
tuning on downstream tasks, motivating the devel-
opment of Parameter-Efficient Fine-Tuning (PEFT)
methods that optimize only a small amount of pa-
rameters while maintaining model performance
(Xu et al., 2023; Han et al., 2024).
Common PEFT approaches include partial fine-
tuning (Ben Zaken et al., 2022; Bu et al., 2024) that
only tune part of the parameters; soft prompt fine-
tuning (Hambardzumyan et al., 2021; Lester et al.,
2021), where trainable prompts are appended to
inputs with model parameters frozen; adapter tun-
ing (Houlsby et al., 2019; Lin et al., 2020; Rücklé
et al., 2021; Karimi Mahabadi et al., 2021; Pfeif-
fer et al., 2021; He et al., 2022; Wang et al., 2022;
Lei et al., 2023) which inserts additional trainablelayers into LLMs and fix the base model parame-
ters; and LoRA(Hu et al., 2022; Aghajanyan et al.,
2021), which decomposes weight updates into low-
rank matrices. Different from other approaches,
LoRA does not change the original model architec-
ture or incurring extra computational cost during
inference since the extra adapters can be merged
into original parameters.
LoRA Initialization. Multiple LoRA initializa-
tion methods have been proposed, with the aim
of improving training efficiency or obtaining other
abilities.
PiSSA (Meng et al., 2024) argued that the de-
fault initialization of “Gaussian noise (He et al.,
2015) and zero” to the adapters can lead to slow
convergence. Hence they propose to apply singu-
lar value decomposition to original weight matri-
ces and utilizes the top components to initialize
LoRA, encapsulating the most significant infor-
mation stored in original weights. CorDA (Yang
et al., 2024) utilizes covariance matrices of data
context, and takes the first (or last) singular vectors
after context-oriented decomposition as initializa-
tion of LoRA adapters. They propose two different
modes, one for improving fine-tuning performance
and the other for mitigating world knowledge for-
getting. Similar to CorDA, EV A (Paischer et al.,
2024) feeds fine-tuning data into the model, ap-
plies sigular value decomposition to activation co-
variance matrices, and takes top singular vectors
as initialization weights. LoRA-GA (Wang et al.,
2024b) also utilizes data context but applies de-
composition on the gradient. (Hayou et al., 2024)
analyze the initialization of LoRA adapters, and
have shown how the asymmetry of two low rank
matrices affects training dynamics.
Harmful Finetuning Attack and Defense strate-
gies. To prevent potential misuse, LLMs usually
undergo specific training to align them with human
values before deployment (Ouyang et al., 2022; Bai
et al., 2022). Nevertheless, jailbreak attacks employ
carefully designed inputs to circumvent this align-
ment, with prominent methods including Greedy
Coordinate Gradient (GCG) (Zou et al., 2023), Au-
toDAN (Liu et al., 2023), and PAIR (Chao et al.,
2023). Beyond these direct attacks, fine-tuning can
also undermine a model’s safety alignment, even
when non-harmful data is used (Qi et al., 2024; He
et al., 2024).
Consequently, researchers have developed var-
ious defense strategies against such fine-tuning
2risks, generally falling into following approaches:
enhancing the original safety alignment (Huang
et al., 2024c,a), restricting the scope of fine-tuning
parameters (Wei et al., 2024; Li et al., 2025), mix-
ing additional safety data (Wang et al., 2024a;
Huang et al., 2024b), modifying the loss function
(Qi et al., 2025) and post-fine-tuning processing
(Yia et al., 2024; Hsu et al., 2024). Different from
previous works, our method focuses on an alter-
native approach of mitigating safety risks during
fine-tuning by modifying initialization and model
architecture, without mixing safety data during fine-
tuning or appending prefix during inference time -
both of which would incur computation overhead.
World Knowledge Forgetting. Catastrophic for-
getting (McCloskey and Cohen, 1989) is a phe-
nomenon when models lose previously acquired
knowledge when adapting to new tasks, and has
been extensively studied in deep learning. Early
approaches to solve the problem include knowl-
edge distillation (Li and Hoiem, 2018), rehearsal
(Riemer et al., 2019) and dynamic architectures
(Yan et al., 2021). For large language models, pre-
serving world knowledge remains challenging due
to massive pre-training data and model size. Recent
efforts mitigate forgetting by freezing pre-trained
layers while introducing new adapters (Wu et al.,
2024; Dou et al., 2024). Recently (Yang et al.,
2024) proposed CorDA with Knowledge-Preserved
Adaptation (KPA) mode, addressing world knowl-
edge forgetting through LoRA initialization.
3 Method
Below, we first review the vanilla LoRA, and de-
scribe our proposed SC-LoRA method.
3.1 LoRA
Following the hypothesis that the update of weight
matrices presents a low rank structure (Aghajanyan
et al., 2021), LoRA (Hu et al., 2022) uses the prod-
uct of two trainable low-rank matrices to learn the
weight change while keeping the original weight
matrices frozen. To express in mathematical form,
LoRA adds low-rank adapters A, B to original
weight matrix W0byW′=W0+BA, where
W′, W0∈Rdout×din,A∈Rr×din,B∈Rdout×r,
r≪min(din, dout). When fine-tuning, W0is kept
frozen, and A, B are trainable parameters.
From the default initialization scheme of LoRA,
Ais initialized by Kaiming Initialization (He et al.,
2015) while Bis initialized by zero matrix. Conse-quently, the adapter term BA=OandW′=W0
at the start of fine-tuning, ensuring the coherence
with the model before fine-tuning. For initial-
izations with non-zero adapter BA(Meng et al.,
2024; Yang et al., 2024; Wang et al., 2024b), the
frozen weights are adjusted to the residual term
Wres=W0−BinitAinit. Then the adapted weight
isW′=W0−BinitAinit+BA=Wres+BA.
In transformer-based LLMs, LoRA adapters are
applied to weight matrices within the self-attention
and multilayer perceptron (MLP) layers.
3.2 SC-LoRA
Known as catastrophic forgetting problem (Chen
et al., 2020), a large language model often per-
forms worse on its pre-trained knowledge after
fine-tuning on a downstream task. To this end,
we consider fine-tuning a large language model on
downstream task T+, while preserving its ability on
the other task T−. Consider the output of a linear
layerh=W0x=Wresx+BinitAinitx. We denote
P+andP−the distribution of hwhen the model
is fed with data from T+andT−, respectively. Our
aim is to initialize A, B within the r-rank constraint
so that BAx preserves the most of P+and the least
ofP−, so that after initialization, the trainable term
BAx is constrained to primarily focus on P+while
avoiding modifying P−. This is equivalent to iden-
tify a low-dimensional subspace S⊂Rdoutwith
rankr, on which the projection of P+is mostly
preserved and the projection of P−is mostly elim-
inated. To evaluate such property of subspace S,
we define the following reward:
Definition 1. For a subspace S⊂Rdoutof dimen-
sionr, define the reward R(S)overP±as:
R(S) = (1 −β)EX+∼P+h
∥ΠS(X+)∥2
2i
−βEX−∼P−h
∥ΠS(X−)∥2
2i
,(1)
where β∈[0,1]is a hyperparameter to tune. Here
ΠS:Rdout→Sdenote the orthogonal projection
operator onto S. See Appendix A.1 for mathemati-
cal definition of ΠS.
The first term of R(S)quantifies the context in-
formation of T+contained in subspace S, while the
second penalizes that of T−. We use βto balance
the trade-off between focusing on T+and preser-
vation on T−. Given the objective to maximize
R(S), in the following we provide Theorem 1 to
compute the optimal subspace and then use it to set
our LoRA initialization scheme.
3(a) LoRA
 (b) SC-LoRA
Figure 1: Comparison of LoRA with default Kaiming initialization and our proposed SC-LoRA. (a) LoRA initializes
down-projection matrix A by Gaussian noise and up-projection matrix B by zero matrix. (b) Our SC-LoRA
initializes A by Q⊤
rW0and B by Qr, where Qrconsists of rorthonormal vectors as columns obtained by Algorithm
1.
Theorem 1. LetCov +,Cov−be the covariance
matrices of random vectors X+∼ P +andX−∼
P−, respectively:
Cov +=Eh
X+X⊤
+i
, (2)
Cov−=Eh
X−X⊤
−i
. (3)
And let
∆Cov = (1 −β)Cov +−βCov−. (4)
Then do eigenvalue decomposition of ∆Cov and
take the first reigenvectors {qi}i∈[r]with the
largest eigenvalues. Then, if following condition
holds, the reward R(S)is maximized:
S= span
{qi}i∈[r]
. (5)
Proof. See Appendix A.2.
Theorem 1 shows the steps to compute the op-
timal subspace that maximized R(S). Then, to
constrain the updating output term BAx in the
subspace S, we propose our LoRA initialization
method:
Binit= (q1q2···qr), (6)
Ainit= (q1q2···qr)⊤W0, (7)
Wres=W0−BinitAinit, (8)
as illustrated in Figure 1b. To explain the initializa-
tion setting, we provide the following theorem:Theorem 2. Leth, x be the output and input of
the original linear layer W0, satisfying h=W0x.
When A, B are initialized by Equations 7, 8, the
following property holds:
BinitAinitx= Π S(h)∈S,∀x∈Rdin.(9)
Proof. See Appendix A.3.
Together with Theorem 1, our initialization
method has the following properties: When β= 0
and the model is fed with data from task T+,hfol-
lows distribution P+, then the norm of the updating
termBAx is maximized, providing the most con-
text information of T+for training; When β= 1
and the model is fed with data from task T−,h
follows distribution P−, then the norm of BAx is
minimized, passing the least context information
ofT−to trainable parameters. When β∈(0,1),
it is the balance between the two cases. The prop-
erty indicates that, during fine-tuning, the trainable
weights are updating more on features related to
T+and less on features related to T−, and hence
enhancing learning T+while avoiding damaging
information related to T−.
The pseudo-code of our initialization algorithm
is shown in Algorithm 1. In practice, it is hard
to format the true distribution and covariance of
output vectors, so we approximate them by feed-
ing hundreds of samples into the model, and use
the collection of output vectors to approximate the
distribution.
4Algorithm 1 SC-LoRA initialization.
Require: Datasets D+,D−from tasks T+, T−, re-
spectively.
1:LetB+=|D+|, B−=|D−|.
2:Separately feed samples in D+,D−into the
pre-trained model, collect batched output
ˆX+∈Rdout×B+,ˆX−∈Rdout×B−of each
linear layer. Within each sample, the output
vector is summed over all tokens.
3:Cov +←1
B+ˆX+ˆX⊤
+.
4:Cov−←1
B−ˆX−ˆX⊤
−.
5:Do eigenvalue decomposition on ∆Cov =
(1−β)Cov +−βCov−, and take the first r
eigenvectors {qi}i∈rwith the largest eigenval-
ues.
6:Qr←(q1q2···qr).
7:Binit←Qr.
8:Ainit←Q⊤
rW0.
9:Wres←W0−BinitAinit.
4 Experiments
In the experiments below, we compare SC-LoRA
with 5 baselines:
(1) Full fine-tuning. Fine-tune on all parameters
of the model;
(2) Vanilla LoRA (Hu et al., 2022). Fine-tune
only on LoRA adapters, with Binitialized with
Gaussian noise (He et al., 2015), and Ainitialized
by zero;
(3) PiSSA (Meng et al., 2024), for efficient fine-
tuning. It applies SVD on pre-trained weight W0
and initializes LoRA adapters by the main parts of
decomposition;
(4) CorDA (Yang et al., 2024) Instruction-
Previewed Adaptation (IPA) mode, for efficient
fine-tuning. It feeds fine-tuning data into the model
to get the covariance of activations, applies self-
defined context-oriented decomposition, and ini-
tializes LoRA adapters with principal directions
obtained in decomposition;
(5) CorDA Knowledge-Preserved Adaptation
(KPA) mode, for knowledge preservation. The
initialization algorithm is basically the same as IPA
mode except that it feeds preserved knowledge data
and take the least principal directions for initializa-
tion.
For initialization of CorDA IPA and KPA mode,
we calculate the covariance matrices with 256 sam-
ples from fine-tuning dataset and preserved knowl-
edge dataset, respectively with 256 samples. Weuse AdamW optimizer (Loshchilov and Hutter,
2019) with the following hyper-parameters: batch
size 128, learning rate 2e-5 (except for experiment
in Section 4.2, where we tune the learning rate of
baselines for better performance), cosine annealing
learning rate schedule, warm-up ratio 0.03, and no
weight decay. The rank of LoRA and its variants
are all set to 128 for comparison. For SC-LoRA,
we tune the hyperparameter βto find a good bal-
anced result. All experiment results are obtained
by running on only one seed.
Below we discuss results in three settings: (1)
Preservation of safety when fine-tuning on benign
data; (2) Preservation of safety when fine-tuning on
poisoned data; (3) Preservation of world knowledge
when fine-tuning on math task.
4.1 Safety Preservation on Benign Finetuning
(Qi et al., 2024) has shown that fine-tuning on be-
nign data can compromise the safety of aligned
LLMs. In this setting, we aim to preserve the
safety of aligned LLM while providing efficient
fine-tuning on downstream tasks. Following the
experimental settings by (Qi et al., 2025), we fine-
tune Llama-2-7b-Chat model with safety alignment
(Touvron et al., 2023) on Samsum (Gliwa et al.,
2019) for 1 epoch. Samsum is a dataset for con-
versation summarization task, containing 14732
training samples.
To initialize our SC-LoRA model, we ran-
domly select 256 samples from fine-tuning dataset
(D+) Samsum to compute covariance matrix Cov +
for each linear layer, then use 256 harmful-
question&refusal-answer pairs (as the safety
dataset D−) provided by (Qi et al., 2025) to com-
puteCov−. These two collections of samples are
also used to compute the covariance matrices of
CorDA IPA and CorDA KPA respectively.
For utility evaluation, we employ the standard
ROUGE-1 score (Lin, 2004) for Samsum. For
safety evaluation, we let the fine-tuned models
to generate answers for 330 malicious questions
provided by (Qi et al., 2024) (distinct from ma-
licious questions for initialization) and employ
DeepSeek-V3 (DeepSeek-AI et al., 2025) API to
judge the harmfulness, assigning each answer an
integer score from 1 (safe) to 5 (most harmful). We
report the average score as harmfulness score of
the model and the fraction of maximum-risk re-
sponses (score = 5) as harmfulness rate . Lower
values for both metrics indicate stronger safety of
the model.
5Method #Params HS↓HR(%) ↓Utility ↑
Llama-2-7b-Chat - 1.100 1.212 24.13
Full fine-tuning 6738M 1.364 5.455 51.41
LoRA 320M 1.176 2.424 50.32
PiSSA 320M 1.252 4.242 51.87
CorDA IPA 320M 1.209 3.333 44.61
CorDA KPA 320M 1.106 0.606 50.89
SC-LoRAβ= 0.5 320M 1.161 1.818 52.54
β= 0.7 320M 1.148 1.818 52.07
β= 0.9 320M 1.097 0.000 51.67
Table 1: Results of Safety preservation and fine-tuning performance when training on benign dataset Samsum.
#Params is the number of trainable parameters. HS and HR denote harmfulness score and harmfulness rate
respectively.
As shown in Table 1, SC-LoRA achieves high
utility, even surpassing full fine-tuning on Samsum
dataset when β= 0.5. At the same time, SC-LoRA
shows almost no safety degradation compared to
the model before fine-tuning, while all baselines
except CorDA KPA present notable safety degrada-
tion, since they are not designed for knowledge
preservation. However, the utilities of all fine-
tuning methods (except for CorDA IPA) are gener-
ally close. We hypothesize that the task of summa-
rization is quite simple, so training for only 1 epoch
is enough for utility convergence. Also, the results
of SC-LoRA shows that when βis increasing, the
safety preservation becomes better while utility is
decreasing. This aligns with our design of βto
balance the trade-off.
4.2 Safety Preservation on Data Poisoning
Attack
Harmful data injection is a common attack method
to degrade the safety of LLMs during fine-tuning
(Huang et al., 2024a,b,c). In this experiment, we
aim to preserve safety in poisoned data scenarios.
To construct the poisoned dataset, we first take
25600 data samples of MetaMathQA (Yu et al.,
2024), then replace 1% of the data by harmful
question-answer pairs provided by (Qi et al., 2024).
We train each method for 1 epoch on the poisoned
dataset. We use 256 samples from MetaMathQA
for the initialization of SC-LoRA and CorDA IPA.
The safety samples used for the initialization of
SC-LoRA and CorDA KPA are the same with the
previous experiment (Section 4.1).
For utility evaluation, we compute the answer
accuracy on the validation set of GSM8k (Cobbe
et al., 2021). Safety evaluation follows the setting
in the previous section 4.1. For better comparabil-ity, we tune the learning rate of LoRA to 2e-5, 5e-5
and 1e-4. The learning rate for other methods is
fixed to 2e-5.
From the results in Table 2, we can observe that
the data points exhibit a wider spread among these
methods, both in utility and safety metric. Com-
pared to the original model, SC-LoRA ( β= 0.9)
exhibits almost no safety degradation, and achieves
best utility, even surpassing full fine-tuning by 3.79
points. When increasing the learning rate, LoRA
shows a sharp decline in safety alignment while
math ability is increasing. LoRA (lr=2e-5) and
CorDA KPA, though preserving safety well, are
insufficient in fine-tuning performance compared
to our method. PiSSA and CorDA IPA, though
showing their capacity in better fine-tuning, heav-
ily degrades the safety of the model. This again
shows the potential of our method to enhance the
utility of the model and preserve safety at the same
time, even when the fine-tuning dataset contains a
small fraction of harmful content. Also, the utility
and safety of SC-LoRA follows the same trend as
in fine-tuning on benign data when βis increasing,
supporting the sedign of our method.
4.3 World Knowledge Preservation
Pre-trained LLMs also have other pre-trained
knowledge that is easy to lose after fine-tuning
on downstream tasks, such as world knowledge
(Yang et al., 2024). In this setting, we aim to pre-
serve the intrinsic world knowledge (e.g., common
sense) within the pre-trained LLM while providing
efficient fine-tuning on downstream tasks. We fine-
tune the Llama-2-7b model (Touvron et al., 2023)
on math task and evaluate its math ability (utility)
and world knowledge performance. We train on
100000 samples of MetaMathQA (Yu et al., 2024)
6Method #Params HS↓HR(%) ↓Utility ↑
Llama-2-7b-Chat - 1.100 1.212 -
Full fine-tuning 6738M 2.248 23.94 41.47
LoRAlr=2e-5 320M 1.118 1.212 31.69
lr=5e-5 320M 2.276 23.64 37.68
lr=1e-4 320M 3.155 41.52 41.93
PiSSA 320M 2.379 29.39 41.77
CorDA IPA 320M 4.239 67.27 43.75
CorDA KPA 320M 1.127 1.212 40.33
SC-LoRAβ= 0.5 320M 1.630 10.91 45.56
β= 0.7 320M 1.224 3.030 45.26
β= 0.9 320M 1.136 1.212 45.26
Table 2: Results of safety preservation and fine-tuning performance when training on poisoned dataset MetaMathQA
with 1% malicious question-answer pairs.
Method #Params TriviaQA ↑NQ-open ↑WebQS ↑Avg↑GSM8k ↑MATH ↑Avg↑
Llama-2-7b - 52.52 18.86 5.86 25.75 - - -
Full fine-tuning 6738M 47.42 4.16 6.64 19.41 50.27 6.94 28.60
LoRA 320M 46.81 1.05 7.04 18.30 41.77 5.46 23.62
PiSSA 320M 47.44 3.32 6.84 19.20 51.63 7.70 29.67
CorDA IPA 320M 30.20 9.83 5.41 15.15 51.40 8.34 29.87
CorDA KPA 320M 46.21 10.64 7.33 21.39 45.03 6.54 25.79
SC-LoRAβ= 0 320M 44.26 5.18 7.19 18.88 53.53 8.98 31.25
β= 0.5 320M 48.91 7.70 6.89 21.17 53.37 8.62 31.00
β= 0.8 320M 50.52 10.64 7.04 22.73 52.46 7.62 30.04
Table 3: Results of world knowledge preservation and math ability after fine-tuning on MetaMATH.
for 1 epoch and evaluate its math ability on GSM8k
(Cobbe et al., 2021) and MATH (Yu et al., 2024)
validation sets. World knowledge is evaluated by
the exact matching score on TriviaQA (Joshi et al.,
2017), NQ-open (Lee et al., 2019), and WebQS
(Berant et al., 2013) through Evaluation-Harness
(Gao et al., 2024). We select 256 random samples
from NQ-open as world knowledge samples used
for the initialization of SC-LoRA and CorDA KPA,
and 256 random samples from MetaMathQA as
fine-tuning dataset for initializing SC-LoRA and
CorDA IPA.
As shown in Table 3, the results of full fine-
tuning and LoRA show the degradation on world
knowledge when fine-tuning on downstream task
MetaMathQA. SC-LoRA achieves best math abil-
ity (surpassing full fine-tuning), and preserves
world knowledge relatively well. When β= 0.8, it
surpasses all baselines on both utility and knowl-
edge preservation. Also, from the results of SC-
LoRA, we can see a clear trend when increasing β,
that the knowledge preservation ability is increas-
ing while the utility is decreasing, which aligns
with our design methodology for βin Section 3.More details will be shown in Section 4.4 to ana-
lyze this trend.
4.4 Experimental Analysis on the
Functionality of Hyper-Parameter β
As explained in Section 3, the value of βbalance
the trade-off between knowledge preservation and
fine-tuning efficiency. Intuitively, when increasing
β, there exists a trend that the fine-tuning perfor-
mance will drop and the knowledge preservation
ability will increase. While we have observed this
trend in the previous section, we illustrate the trend
more explicitly in Figure 2 and 3. In Figure 2, both
two curves shows knowledge preservation improve-
ment when βis increasing: one for safety increas-
ing, and the other for world knowledge preserva-
tion improvement. In Figure 3, the math ability
decreases when βis increasing, aligning with our
expectations. The utility on Samsum, however,
does not show evident trend as βvaries, but fluc-
tuating around 0.52. We hypothesize that the task
of summerization is quite simple, so whatever the
value of β, it is sufficient for utility convergence
during fine-tuning.
7Figure 2: Relations between βand knowledge preservation performance. The experiment setting of the left figure
is described in Section 4.1, while that of the right figure is described in Section 4.3. Lower harmfulness score or
higher world knowledge score indicates better performance on knowledge preservation.
Figure 3: Relations between βand fine-tuning performance. The experiment setting of the left figure is described in
Section 4.1, while that of the right figure is conducted in Section 4.3. The right figure shows clear monotonicity
withβ, while such trend does not occur in the left figure.
These trends give experimental support to our
method design, that by adjusting βwe can balance
the trade-off. Interestingly, a linear relationship
was observed between βvalues and knowledge
preservation in some experimental settings.
5 Conclusion
Aimed to balance the trade-off between efficient
fine-tuning and knowledge preservation, this paper
presents a data-driven LoRA initialization that uti-
lizes the subspace constrain, in order to strengthen
the target knowledge while downgrading its influ-
ence on preserved knowledge. Theoretical analysis
are provided to support our method, including the
choice of subspace and the initialization setting.
We conduct extensive experiments regrading safety
preservation and world knowledge preservation,
during fine-tuning on various downstream tasks
such as math and summarization. The results of
experiments strongly demonstrate that our method
can not only promote fine-tuning performance on
downstream tasks, but also preserve the intrinsic
knowledge stored in pre-trained model, surpassingcontemporary LoRA initialization methods.
6 Limitations
Although SC-LoRA can successfully handle both
efficient fine-tuning and knowledge preservation at
the same time, it still has drawbacks.
First, SC-LoRA is just a LoRA initialization
method, and does not strongly constrain the updates
during fine-tuning process. Hence after fine-tuning
on more complex tasks and with more steps, the
knowledge preservation ability can also drop (see
the preservation drop of NQ-open in Table 3 for
example).
Second, its application on preserving other types
of knowledge remains unexplored. Future work
may consider applying SC-LoRA to preserving
multimodal large language model’s performance
on pre-training tasks (Zhai et al., 2024) or large
language model’s reasoning ability.
These aspects provide promising directions for
future researches.
8Acknowledgments
We are grateful to Prof. Jingzhao Zhang from Ts-
inghua University for helpful discussions and con-
structive comments on this work.
References
Armen Aghajanyan, Sonal Gupta, and Luke Zettle-
moyer. 2021. Intrinsic dimensionality explains the
effectiveness of language model fine-tuning. In Pro-
ceedings of the 59th Annual Meeting of the Associa-
tion for Computational Linguistics and the 11th Inter-
national Joint Conference on Natural Language Pro-
cessing (Volume 1: Long Papers) , pages 7319–7328,
Online. Association for Computational Linguistics.
Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda
Askell, Anna Chen, Nova DasSarma, Dawn Drain,
Stanislav Fort, Deep Ganguli, Tom Henighan, and 1
others. 2022. Training a helpful and harmless assis-
tant with reinforcement learning from human feed-
back. arXiv preprint arXiv:2204.05862 .
Elad Ben Zaken, Yoav Goldberg, and Shauli Ravfogel.
2022. BitFit: Simple parameter-efficient fine-tuning
for transformer-based masked language-models. In
Proceedings of the 60th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 2:
Short Papers) , pages 1–9, Dublin, Ireland. Associa-
tion for Computational Linguistics.
Jonathan Berant, Andrew Chou, Roy Frostig, and Percy
Liang. 2013. Semantic parsing on Freebase from
question-answer pairs. In Proceedings of the 2013
Conference on Empirical Methods in Natural Lan-
guage Processing , pages 1533–1544, Seattle, Wash-
ington, USA. Association for Computational Linguis-
tics.
Zhiqi Bu, Yu-Xiang Wang, Sheng Zha, and George
Karypis. 2024. Differentially private bias-term fine-
tuning of foundation models. In Proceedings of the
41st International Conference on Machine Learning ,
volume 235 of Proceedings of Machine Learning
Research , pages 4730–4751. PMLR.
Patrick Chao, Alexander Robey, Edgar Dobriban,
Hamed Hassani, George J Pappas, and Eric Wong.
2023. Jailbreaking black box large language models
in twenty queries. arXiv preprint arXiv:2310.08419 .
Sanyuan Chen, Yutai Hou, Yiming Cui, Wanxiang Che,
Ting Liu, and Xiangzhan Yu. 2020. Recall and learn:
Fine-tuning deep pretrained language models with
less forgetting. In Proceedings of the 2020 Confer-
ence on Empirical Methods in Natural Language
Processing (EMNLP) , pages 7870–7881, Online. As-
sociation for Computational Linguistics.
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian,
Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias
Plappert, Jerry Tworek, Jacob Hilton, Reiichiro
Nakano, Christopher Hesse, and John Schulman.2021. Training verifiers to solve math word prob-
lems. Preprint , arXiv:2110.14168.
DeepSeek-AI, Aixin Liu, Bei Feng, Bing Xue, Bingx-
uan Wang, Bochao Wu, Chengda Lu, Chenggang
Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan,
Damai Dai, Daya Guo, Dejian Yang, Deli Chen,
Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai,
and 181 others. 2025. Deepseek-v3 technical report.
Preprint , arXiv:2412.19437.
Shihan Dou, Enyu Zhou, Yan Liu, Songyang Gao, Wei
Shen, Limao Xiong, Yuhao Zhou, Xiao Wang, Zhi-
heng Xi, Xiaoran Fan, Shiliang Pu, Jiang Zhu, Rui
Zheng, Tao Gui, Qi Zhang, and Xuanjing Huang.
2024. LoRAMoE: Alleviating world knowledge for-
getting in large language models via MoE-style plu-
gin. In Proceedings of the 62nd Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 1: Long Papers) , pages 1932–1945, Bangkok,
Thailand. Association for Computational Linguistics.
Leo Gao, Jonathan Tow, Baber Abbasi, Stella Bider-
man, Sid Black, Anthony DiPofi, Charles Foster,
Laurence Golding, Jeffrey Hsu, Alain Le Noac’h,
Haonan Li, Kyle McDonell, Niklas Muennighoff,
Chris Ociepa, Jason Phang, Laria Reynolds, Hailey
Schoelkopf, Aviya Skowron, Lintang Sutawika, and
5 others. 2024. A framework for few-shot language
model evaluation.
Bogdan Gliwa, Iwona Mochol, Maciej Biesek, and Alek-
sander Wawer. 2019. SAMSum corpus: A human-
annotated dialogue dataset for abstractive summa-
rization. In Proceedings of the 2nd Workshop on
New Frontiers in Summarization , pages 70–79, Hong
Kong, China. Association for Computational Linguis-
tics.
Ian J. Goodfellow, Mehdi Mirza, Da Xiao, Aaron
Courville, and Yoshua Bengio. 2015. An empirical
investigation of catastrophic forgetting in gradient-
based neural networks. Preprint , arXiv:1312.6211.
Karen Hambardzumyan, Hrant Khachatrian, and
Jonathan May. 2021. WARP: Word-level Adversarial
ReProgramming. In Proceedings of the 59th Annual
Meeting of the Association for Computational Lin-
guistics and the 11th International Joint Conference
on Natural Language Processing (Volume 1: Long
Papers) , pages 4921–4933, Online. Association for
Computational Linguistics.
Zeyu Han, Chao Gao, Jinyang Liu, Jeff Zhang, and
Sai Qian Zhang. 2024. Parameter-efficient fine-
tuning for large models: A comprehensive survey.
Transactions on Machine Learning Research .
Soufiane Hayou, Nikhil Ghosh, and Bin Yu. 2024. The
impact of initialization on lora finetuning dynam-
ics. In Advances in Neural Information Processing
Systems , volume 37, pages 117015–117040. Curran
Associates, Inc.
Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-
Kirkpatrick, and Graham Neubig. 2022. Towards a
9unified view of parameter-efficient transfer learning.
InInternational Conference on Learning Representa-
tions .
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun. 2015. Delving deep into rectifiers: Surpassing
human-level performance on imagenet classification.
InProceedings of the IEEE International Conference
on Computer Vision (ICCV) .
Luxi He, Mengzhou Xia, and Peter Henderson. 2024.
What is in your safe data? identifying benign data
that breaks safety. In First Conference on Language
Modeling .
Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,
Bruna Morrone, Quentin De Laroussilhe, Andrea
Gesmundo, Mona Attariyan, and Sylvain Gelly. 2019.
Parameter-efficient transfer learning for NLP. In
Proceedings of the 36th International Conference
on Machine Learning , volume 97 of Proceedings
of Machine Learning Research , pages 2790–2799.
PMLR.
Chia-Yi Hsu, Yu-Lin Tsai, Chih-Hsun Lin, Pin-Yu Chen,
Chia-Mu Yu, and Chun-Ying Huang. 2024. Safe
loRA: The silver lining of reducing safety risks when
finetuning large language models. In The Thirty-
eighth Annual Conference on Neural Information
Processing Systems .
Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-
Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu
Chen. 2022. Lora: Low-rank adaptation of large
language models. In International Conference on
Learning Representations .
Tiansheng Huang, Sihao Hu, Fatih Ilhan, Selim Furkan
Tekin, and Ling Liu. 2024a. Booster: Tackling
harmful fine-tuning for large language models via
attenuating harmful perturbation. arXiv preprint
arXiv:2409.01586 .
Tiansheng Huang, Sihao Hu, Fatih Ilhan, Selim Furkan
Tekin, and Ling Liu. 2024b. Lazy safety align-
ment for large language models against harmful fine-
tuning. arXiv preprint arXiv:2405.18641 , 2.
Tiansheng Huang, Sihao Hu, and Ling Liu. 2024c. Vac-
cine: Perturbation-aware alignment for large lan-
guage models against harmful fine-tuning attack.
arXiv preprint arXiv:2402.01109 .
Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke
Zettlemoyer. 2017. TriviaQA: A large scale distantly
supervised challenge dataset for reading comprehen-
sion. In Proceedings of the 55th Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 1: Long Papers) , pages 1601–1611, Vancouver,
Canada. Association for Computational Linguistics.
Rabeeh Karimi Mahabadi, James Henderson, and Se-
bastian Ruder. 2021. Compacter: Efficient low-rank
hypercomplex adapter layers. In Advances in Neural
Information Processing Systems , volume 34, pages
1022–1035. Curran Associates, Inc.Kenton Lee, Ming-Wei Chang, and Kristina Toutanova.
2019. Latent retrieval for weakly supervised open
domain question answering. In Proceedings of the
57th Annual Meeting of the Association for Computa-
tional Linguistics , pages 6086–6096, Florence, Italy.
Association for Computational Linguistics.
Tao Lei, Junwen Bai, Siddhartha Brahma, Joshua
Ainslie, Kenton Lee, Yanqi Zhou, Nan Du, Vin-
cent Zhao, Yuexin Wu, Bo Li, Yu Zhang, and Ming-
Wei Chang. 2023. Conditional adapters: Parameter-
efficient transfer learning with fast inference. In Ad-
vances in Neural Information Processing Systems ,
volume 36, pages 8152–8172. Curran Associates,
Inc.
Brian Lester, Rami Al-Rfou, and Noah Constant. 2021.
The power of scale for parameter-efficient prompt
tuning. In Proceedings of the 2021 Conference on
Empirical Methods in Natural Language Processing ,
pages 3045–3059, Online and Punta Cana, Domini-
can Republic. Association for Computational Lin-
guistics.
Shen Li, Liuyi Yao, Lan Zhang, and Yaliang Li. 2025.
Safety layers in aligned large language models: The
key to LLM security. In The Thirteenth International
Conference on Learning Representations .
Zhizhong Li and Derek Hoiem. 2018. Learning without
forgetting. IEEE Transactions on Pattern Analysis
and Machine Intelligence , 40(12):2935–2947.
Chin-Yew Lin. 2004. ROUGE: A package for auto-
matic evaluation of summaries. In Text Summariza-
tion Branches Out , pages 74–81, Barcelona, Spain.
Association for Computational Linguistics.
Zhaojiang Lin, Andrea Madotto, and Pascale Fung.
2020. Exploring versatile generative language model
via parameter-efficient transfer learning. In Find-
ings of the Association for Computational Linguistics:
EMNLP 2020 , pages 441–459, Online. Association
for Computational Linguistics.
Xiaogeng Liu, Nan Xu, Muhao Chen, and Chaowei
Xiao. 2023. Autodan: Generating stealthy jailbreak
prompts on aligned large language models. arXiv
preprint arXiv:2310.04451 .
Ilya Loshchilov and Frank Hutter. 2019. De-
coupled weight decay regularization. Preprint ,
arXiv:1711.05101.
Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jian-
Guang Lou, Chongyang Tao, Xiubo Geng, Qingwei
Lin, Shifeng Chen, Yansong Tang, and Dongmei
Zhang. 2025. Wizardmath: Empowering mathemat-
ical reasoning for large language models via rein-
forced evol-instruct. In The Thirteenth International
Conference on Learning Representations .
M. McCloskey and N. J. Cohen. 1989. Catastrophic
interference in connectionist networks: The sequen-
tial learning problem. Psychology of learning and
motivation , 24:109–165.
10Fanxu Meng, Zhaohui Wang, and Muhan Zhang. 2024.
Pissa: Principal singular values and singular vectors
adaptation of large language models. In Advances in
Neural Information Processing Systems , volume 37,
pages 121038–121072. Curran Associates, Inc.
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,
Carroll Wainwright, Pamela Mishkin, Chong Zhang,
Sandhini Agarwal, Katarina Slama, Alex Ray, and 1
others. 2022. Training language models to follow in-
structions with human feedback. Advances in neural
information processing systems , 35:27730–27744.
Fabian Paischer, Lukas Hauzenberger, Thomas
Schmied, Benedikt Alkin, Marc Peter Deisenroth,
and Sepp Hochreiter. 2024. One initialization to rule
them all: Fine-tuning via explained variance adapta-
tion. In NeurIPS 2024 Workshop on Fine-Tuning in
Modern Machine Learning: Principles and Scalabil-
ity.
Jonas Pfeiffer, Aishwarya Kamath, Andreas Rücklé,
Kyunghyun Cho, and Iryna Gurevych. 2021.
AdapterFusion: Non-destructive task composition
for transfer learning. In Proceedings of the 16th Con-
ference of the European Chapter of the Association
for Computational Linguistics: Main Volume , pages
487–503, Online. Association for Computational Lin-
guistics.
Xiangyu Qi, Ashwinee Panda, Kaifeng Lyu, Xiao Ma,
Subhrajit Roy, Ahmad Beirami, Prateek Mittal, and
Peter Henderson. 2025. Safety alignment should be
made more than just a few tokens deep. In The Thir-
teenth International Conference on Learning Repre-
sentations .
Xiangyu Qi, Yi Zeng, Tinghao Xie, Pin-Yu Chen, Ruoxi
Jia, Prateek Mittal, and Peter Henderson. 2024. Fine-
tuning aligned language models compromises safety,
even when users do not intend to! In The Twelfth In-
ternational Conference on Learning Representations .
Matthew Riemer, Ignacio Cases, Robert Ajemian, Miao
Liu, Irina Rish, Yuhai Tu, , and Gerald Tesauro. 2019.
Learning to learn without forgetting by maximizing
transfer and minimizing interference. In Interna-
tional Conference on Learning Representations .
Andreas Rücklé, Gregor Geigle, Max Glockner, Tilman
Beck, Jonas Pfeiffer, Nils Reimers, and Iryna
Gurevych. 2021. AdapterDrop: On the efficiency
of adapters in transformers. In Proceedings of the
2021 Conference on Empirical Methods in Natural
Language Processing , pages 7930–7946, Online and
Punta Cana, Dominican Republic. Association for
Computational Linguistics.
Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
bert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti
Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton
Ferrer, Moya Chen, Guillem Cucurull, David Esiobu,
Jude Fernandes, Jeremy Fu, Wenyin Fu, and 49 oth-
ers. 2023. Llama 2: Open foundation and fine-tuned
chat models. Preprint , arXiv:2307.09288.Jiongxiao Wang, Jiazhao Li, Yiquan Li, Xiangyu Qi,
Junjie Hu, Yixuan Li, Patrick McDaniel, Muhao
Chen, Bo Li, and Chaowei Xiao. 2024a. Back-
dooralign: Mitigating fine-tuning based jailbreak at-
tack with backdoor enhanced safety alignment. In
The Thirty-eighth Annual Conference on Neural In-
formation Processing Systems .
Shaowen Wang, Linxi Yu, and Jian Li. 2024b. Lora-ga:
Low-rank adaptation with gradient approximation. In
Advances in Neural Information Processing Systems ,
volume 37, pages 54905–54931. Curran Associates,
Inc.
Yaqing Wang, Sahaj Agarwal, Subhabrata Mukherjee,
Xiaodong Liu, Jing Gao, Ahmed Hassan Awadal-
lah, and Jianfeng Gao. 2022. AdaMix: Mixture-
of-adaptations for parameter-efficient model tuning.
InProceedings of the 2022 Conference on Empiri-
cal Methods in Natural Language Processing , pages
5744–5760, Abu Dhabi, United Arab Emirates. As-
sociation for Computational Linguistics.
Boyi Wei, Kaixuan Huang, Yangsibo Huang, Tinghao
Xie, Xiangyu Qi, Mengzhou Xia, Prateek Mittal,
Mengdi Wang, and Peter Henderson. 2024. Assess-
ing the brittleness of safety alignment via pruning
and low-rank modifications. In Proceedings of the
41st International Conference on Machine Learning ,
volume 235 of Proceedings of Machine Learning
Research , pages 52588–52610. PMLR.
Chengyue Wu, Yukang Gan, Yixiao Ge, Zeyu Lu, Jia-
hao Wang, Ye Feng, Ying Shan, and Ping Luo. 2024.
LLaMA pro: Progressive LLaMA with block expan-
sion. In Proceedings of the 62nd Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 1: Long Papers) , pages 6518–6537, Bangkok,
Thailand. Association for Computational Linguistics.
Lingling Xu, Haoran Xie, Si-Zhao Joe Qin, Xiaohui
Tao, and Fu Lee Wang. 2023. Parameter-efficient
fine-tuning methods for pretrained language mod-
els: A critical review and assessment. Preprint ,
arXiv:2312.12148.
Shipeng Yan, Jiangwei Xie, and Xuming He. 2021. Der:
Dynamically expandable representation for class in-
cremental learning. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recog-
nition (CVPR) , pages 3014–3023.
Yibo Yang, Xiaojie Li, Zhongzhu Zhou, Shuaiwen Leon
Song, Jianlong Wu, Liqiang Nie, and Bernard
Ghanem. 2024. Corda: Context-oriented decompo-
sition adaptation of large language models for task-
aware parameter-efficient fine-tuning. In Advances in
Neural Information Processing Systems , volume 37,
pages 71768–71791. Curran Associates, Inc.
Xin Yia, Shunfan Zheng, Linlin Wang, Xiaoling Wang,
and Liang He. 2024. A safety realignment frame-
work via subspace-oriented model fusion for large
language models.
11Longhui Yu, Weisen Jiang, Han Shi, Jincheng YU,
Zhengying Liu, Yu Zhang, James Kwok, Zhenguo Li,
Adrian Weller, and Weiyang Liu. 2024. Metamath:
Bootstrap your own mathematical questions for large
language models. In The Twelfth International Con-
ference on Learning Representations .
Yuexiang Zhai, Shengbang Tong, Xiao Li, Mu Cai, Qing
Qu, Yong Jae Lee, and Yi Ma. 2024. Investigating the
catastrophic forgetting in multimodal large language
model fine-tuning. In Conference on Parsimony and
Learning , volume 234 of Proceedings of Machine
Learning Research , pages 202–227. PMLR.
Andy Zou, Zifan Wang, Nicholas Carlini, Milad Nasr,
J Zico Kolter, and Matt Fredrikson. 2023. Univer-
sal and transferable adversarial attacks on aligned
language models. arXiv preprint arXiv:2307.15043 .
12A Definitions and Proofs
A.1 Mathematical Definition of ΠS
Definition 2. Suppose Sis a subspace of Rnof
dimension r, and let {qi}i∈[r]be an orthonormal
basis of S, then the orthogonal projection operator
ontoS, denoted ΠS, is defined as:
ΠS(x) =rX
i=1(q⊤
ix)qi
=rX
i=1(qiq⊤
i)x,∀x∈Rn.(10)
Note: the selection of the orthonormal basis does
not affect ΠS.
A.2 Proof for Theorem 1
Proof. Suppose for some subspace S⊂Rd(ignore
the subscript of doutfor simplicity) of dimension
r, there exists an orthonormal basis {vi}i∈[r]that
spans S, that is S= span
{vi}i∈[r]
.
For simplicity, denote
˜Ir=X
i∈[r]viv⊤
i, (11)
then the following equality holds:
˜I⊤
r˜Ir=X
i∈[r]X
j∈[r]viv⊤
ivjv⊤
j
=X
i∈[r]X
j∈[r]vi⟨vi, vj⟩v⊤
j
=X
i∈[r]X
j∈[r]δijviv⊤
j
=X
i∈[r]viv⊤
i=˜Ir.(12)
From property of projection,
ΠS(X±) =rX
i=1⟨X±, vi⟩vi=rX
i=1viv⊤
iX±
= rX
i=1viv⊤
i!
X±=˜IrX±.(13)
ThusEX±∼P±h
∥ΠS(X±)∥2
2i
=EX±∼P±˜IrX±2
2
=EX±∼P±h
tr
X⊤
±˜I⊤
r˜IrX±i
=EX±∼P±h
tr
X⊤
±˜IrX±i
=EX±∼P±h
tr
˜IrX±X⊤
±i
=tr
˜IrEX±∼P±h
X±X⊤
±i
=tr
˜IrCov±
.(14)
Suppose the spectral decomposition of (1−
β)Cov( X+)−βCov(X−)isQΣQ⊤, where Q=
(q1q2···qd),Σis diagonal with eigenvalues
sorted in descending order. Then we have
R(S) = (1 −β)EX+∼P+h
∥ΠS(X+)∥2
2i
−βEX−∼P−h
∥ΠS(X−)∥2
2i
= (1−β)tr
˜IrCov +
−βtr
˜IrCov−
= tr
˜Ir∆Cov
=X
i∈[r]tr
viv⊤
iQΣQ⊤
=X
i∈[r]v⊤
iQΣQ⊤vi.
(15)
Extend {vi}i∈[r]to a complete orthonormal basis
{vi}i∈[d]forRd, and denote ui=Q⊤
ivi. Since Qis
an orthogonal matrix, {ui}i∈[d]is also an orthonor-
mal basis for Rd. From Ky Fan’s theorem on eigen-
values, maxP
i∈[r]v⊤
iQΣQ⊤vi
=P
i∈[r]Σii,
and one can easily verify that the condition above
achieves the maximum.
For the if and only if part (adding the condition
of eigenvalue gap): suppose U= (u1u2···ud)⊤
as an orthogonal matrix, then
R({vi}i∈[r]) =X
i∈[r]u⊤
iΣui
=X
i∈[r]dX
j=1ΣjjU2
ij
=dX
j=1
ΣjjX
i∈[r]U2
ij
.(16)
13From property of orthogonal matrix,P
i∈[r]U2
ij≤1andPd
j=1P
i∈[r]U2
ij=r,
then to maximize R, from the additional assump-
tion we needP
i∈[r]U2
ij=(
1,1≤j≤r
0, r+ 1≤j≤d,
which is equivalent to
U⊤
1:r,1:dU1:r,1:d=IrO
O O
. (17)
From U1:r,1:d= (v1v2···vr)⊤Q, we know
that this is also equivalent to
(v1v2···vr)(v1v2···vr)⊤=QIrO
O O
Q⊤,
(18)
which is also written as
rX
i=1viv⊤
i=rX
i=1qiq⊤
i. (19)
Indicating S= span
{qi}i∈[r]
.
A.3 Proof of Theorem 2
Proof. Denote Qr= (q1q2···qr).
Since{qi}i∈[r]is a orthonormal basis that spans
S, from definition of orthogonal projection we have
ΠS(h) =rX
i=1qiq⊤
ih=QrQ⊤
rh. (20)
Thus∀x∈Rdin, we have
BinitAinitx=QrQ⊤
rW0x=QrQ⊤
rh= Π S(h),
(21)
which completes the proof.
B Numerical instability in sparse sample
setting
When the sample size is much larger than the output
activation dimension, min(|D+|,|D−|)≫dout,
setting β∈[0,1]causes no issue. However,
when samples are sparse (specifically, when the
number of negative-task sample N−< d out−r,
setting β= 1 introduces multiple valid solu-
tions in the spectral decomposition step due to
high-dimensional freedom in the null space of
Cov−. Mathematically, the rank of Cov−is at
most N−, resulting in a null space of dimensiondout−rank(Cov −)≥dout−N−> r. Conse-
quently, any arbitrary set of rorthonormal vec-
torsin this null space can satisfy the decomposition
criterion, leading to non-unique initialization of pa-
rameters AandB.
To mitigate this instability, we recommend set-
ting1−βto a small positive value (rather than
exactly zero). This retains the regularization from
Cov +in the objective function, which constrains
the null space ambiguity and stabilizes the spectral
decomposition, empirically improves fine-tuning
performance.
14