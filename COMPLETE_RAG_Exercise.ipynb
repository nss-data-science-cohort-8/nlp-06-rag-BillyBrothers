{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a0842ba-6601-460c-ad3a-d07bad5565d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "from langchain_huggingface import HuggingFaceEmbeddings, HuggingFacePipeline\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "import os\n",
    "os.environ[\"OPENAI_API_BASE\"] = \"https://openrouter.ai/api/v1\"\n",
    "\n",
    "import json\n",
    "with open(\"keys.json\", \"r\") as fi:\n",
    "    api_key = json.load(fi)['api_key']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61917709-5141-433f-990b-0bf19659599c",
   "metadata": {},
   "source": [
    "## Part 1: Manual RAG System\n",
    "\n",
    "First, we'll put together the components of our RAG system individually.\n",
    "\n",
    "We'll start with our data source. We'll use FAISS for our vector database.\n",
    "\n",
    "For this exercise, we'll be working with a recent research article, [SC-LoRA: Balancing Efficient Fine-tuning and Knowledge Preservation via Subspace-Constrained LoRA](https://arxiv.org/abs/2505.23724). The text of this article is contained in the txts directory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3fbdb8-a9a5-423a-a3a3-8b50db55de9a",
   "metadata": {},
   "source": [
    "Our goal is to store passages from this text in our database. We'll use the RecursiveCharacterTextSplitter, which will divide the text into chunks of length <= 550, where chunks overlap by 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35b9baef-4a05-4a9b-9402-8f55c664d05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/2505.23724v1.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "chunks = text_splitter.split_text(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf80c790-69f1-455b-a668-972832ad292c",
   "metadata": {},
   "source": [
    "Now, we need to create an embedding of these chunks. We can use the all-MiniLM-L6-v2 embedder for this. \n",
    "\n",
    "**Task 2:** Use a sentence transformer to encode all of the chunks. Then save the results in a faiss IndexFlatIP index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "875843db-056e-4e82-abe5-003399250beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "embeddings = embedder.encode(chunks)\n",
    "\n",
    "dimension = embeddings.shape[1]\n",
    "index = faiss.IndexFlatIP(dimension)\n",
    "index.add(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f6c8af-c046-4ee5-92f4-f9f8e2cdb165",
   "metadata": {},
   "source": [
    "The next necessary piece is a generative model. We'll make use of [OpenRouter](https://openrouter.ai/), using the OpenAI interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c609d53-37ff-4dc0-9beb-169fe4e3c19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    "    api_key=api_key,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a05b431c-47df-493d-9ad0-219da4cb08f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How does SC-LoRA differ from regular LoRA?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93ad83f",
   "metadata": {},
   "source": [
    "Baseline LLM Query\n",
    "The notebook sets up an OpenAI-compatible client for OpenRouter and queries the LLM directly with a question (e.g., \"How does SC-LoRA differ from regular LoRA?\") without any context.\n",
    "This demonstrates how the LLM answers based only on its pretraining, not the specific paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0573c3a7-da04-45c9-be65-6ccd24385468",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"SC-LoRA (Space-Conditioned Low-Rank Adaptation) and LoRA (Low-Rank Adaptation) are both methods used in the context of adapting large pre-trained models to specific tasks or domains with efficient and effective fine-tuning strategies. While they share the common goal of updating a subset of model parameters to adapt to new conditions (like tasks, styles, or domains) without retraining the entire model, there are key differences between SC-LoRA and regular LoRA:\\n\\n1. **Basic Approach**:\\n   - **LoRA**: This method works by adding low-rank matrices to the original weights of the model layers. These low-rank matrices are learned during fine-tuning and allow the model to adapt to new tasks with a relatively small number of additional parameters. LoRA focuses on optimizing these adaptation matrices with the goal of minimizing the need for extensive retraining of the entire model.\\n   - **SC-LoRA**: SC-LoRA extends the basic LoRA approach by incorporating an additional conditioning mechanism. It not only adapts the model to new tasks through low-rank updates but also considers a conditioning space that can further modulate the adaptation. This conditioning space allows for more flexible and potentially more precise control over how the adaptation is performed, especially in scenarios where task-specific or domain-specific nuances need to be captured.\\n\\n2. **Adaptation Mechanism**:\\n   - **LoRA**: The adaptation in LoRA is primarily based on the learned low-rank matrices that directly influence the model's weights. The adaptation is inherently tied to the parameters of these matrices.\\n   - **SC-LoRA**: SC-LoRA introduces an additional layer of abstraction by conditioning the adaptation on a set of space-conditioned factors. This means that the adaptation not only depends on the task or domain but can also be influenced by other factors that are encoded in the conditioning space. This allows for a more dynamic and potentially richer form of adaptation.\\n\\n3. **Use Cases and Flexibility**:\\n   - **LoRA**: LoRA is particularly useful for efficiently adapting large models to new tasks or domains with minimal overhead in terms of additional parameters and computation. It's suitable for a wide range of applications where some level of domain shift or task change needs to be accommodated.\\n   - **SC-LoRA**: SC-LoRA, with its conditioning mechanism, offers greater flexibility and potentially improved performance in scenarios where the adaptation needs to capture complex relationships between different conditions (tasks, domains, styles, etc.). This could be particularly beneficial in multi-task learning scenarios, continual learning, or when dealing with highly variable data.\\n\\n4. **Parameter Efficiency and Scalability**:\\n   - Both methods aim to be parameter-efficient, but SC-LoRA might introduce additional parameters for the conditioning space, which could affect its scalability compared to LoRA. However, the benefits of more flexible adaptation might outweigh the costs in certain applications.\\n\\nIn summary, while LoRA provides an efficient way to adapt large models through low-rank updates, SC-LoRA extends this concept with an additional conditioning mechanism, offering potentially more flexible and nuanced adaptations to different tasks or domains. The choice between them would depend on the specific requirements of the application and the nature of the adaptation needed.\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"meta-llama/llama-4-scout:free\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": query,\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0a9843-c36c-409d-b87d-b2b168f58720",
   "metadata": {},
   "source": [
    "**Task 3:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06e7087d-0223-4777-94e2-9bb64924ed6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How does SC-LoRA differ from regular LoRA?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "342a54d6-07c0-43ae-95fb-5060fc5b1076",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_embedding = embedder.encode([query])\n",
    "D, I = index.search(query_embedding, k=5)\n",
    "most_similar_chunks = I[0]\n",
    "\n",
    "context = \"\"\n",
    "for i in most_similar_chunks:\n",
    "    context += \"\\n\\n\" + chunks[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "567d1676-fa5b-4443-970e-c07ebe8c24e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = (\n",
    "    \"Use the given context to answer the question. \"\n",
    "    \"If you don't know the answer, say you don't know. \"\n",
    "    \"Use three sentences maximum and keep the answer concise. \"\n",
    "    f\"Context: {context}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7263b9",
   "metadata": {},
   "source": [
    "The notebook then:\n",
    "Encodes the query.\n",
    "Retrieves the top-5 most similar text chunks from the FAISS index.\n",
    "Concatenates these chunks as context.\n",
    "Constructs a system_prompt that instructs the LLM to answer using only the provided context.\n",
    "The LLM is then queried again, but this time with the context included as a system prompt.\n",
    "Improvement: The answer is now grounded in the actual content of the paper, making it more accurate and reliable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "22146d92-f899-423b-8126-533143681072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SC-LoRA is a LoRA initialization method that modifies the base LoRA approach. The key difference is that SC-LoRA introduces a hyperparameter β to balance utility and safety, allowing for better preservation of safety and knowledge during fine-tuning. This results in SC-LoRA outperforming regular LoRA in both safety and utility metrics.\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"meta-llama/llama-4-scout:free\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": system_prompt,\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": query,\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ea8c07ba-8619-46fd-a62f-8090f94674e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "methods, both in utility and safety metric. Com-\n",
      "pared to the original model, SC-LoRA ( β= 0.9)\n",
      "exhibits almost no safety degradation, and achieves\n",
      "best utility, even surpassing full fine-tuning by 3.79\n",
      "points. When increasing the learning rate, LoRA\n",
      "shows a sharp decline in safety alignment while\n",
      "math ability is increasing. LoRA (lr=2e-5) and\n",
      "CorDA KPA, though preserving safety well, are\n",
      "insufficient in fine-tuning performance compared\n",
      "to our method. PiSSA and CorDA IPA, though\n",
      "\n",
      "sponses (score = 5) as harmfulness rate . Lower\n",
      "values for both metrics indicate stronger safety of\n",
      "the model.\n",
      "5Method #Params HS↓HR(%) ↓Utility ↑\n",
      "Llama-2-7b-Chat - 1.100 1.212 24.13\n",
      "Full fine-tuning 6738M 1.364 5.455 51.41\n",
      "LoRA 320M 1.176 2.424 50.32\n",
      "PiSSA 320M 1.252 4.242 51.87\n",
      "CorDA IPA 320M 1.209 3.333 44.61\n",
      "CorDA KPA 320M 1.106 0.606 50.89\n",
      "SC-LoRAβ= 0.5 320M 1.161 1.818 52.54\n",
      "β= 0.7 320M 1.148 1.818 52.07\n",
      "β= 0.9 320M 1.097 0.000 51.67\n",
      "\n",
      "2019) with the following hyper-parameters: batch\n",
      "size 128, learning rate 2e-5 (except for experiment\n",
      "in Section 4.2, where we tune the learning rate of\n",
      "baselines for better performance), cosine annealing\n",
      "learning rate schedule, warm-up ratio 0.03, and no\n",
      "weight decay. The rank of LoRA and its variants\n",
      "are all set to 128 for comparison. For SC-LoRA,\n",
      "we tune the hyperparameter βto find a good bal-\n",
      "anced result. All experiment results are obtained\n",
      "by running on only one seed.\n",
      "\n",
      "Although SC-LoRA can successfully handle both\n",
      "efficient fine-tuning and knowledge preservation at\n",
      "the same time, it still has drawbacks.\n",
      "First, SC-LoRA is just a LoRA initialization\n",
      "method, and does not strongly constrain the updates\n",
      "during fine-tuning process. Hence after fine-tuning\n",
      "on more complex tasks and with more steps, the\n",
      "knowledge preservation ability can also drop (see\n",
      "the preservation drop of NQ-open in Table 3 for\n",
      "example).\n",
      "Second, its application on preserving other types\n",
      "\n",
      "LoRA 320M 46.81 1.05 7.04 18.30 41.77 5.46 23.62\n",
      "PiSSA 320M 47.44 3.32 6.84 19.20 51.63 7.70 29.67\n",
      "CorDA IPA 320M 30.20 9.83 5.41 15.15 51.40 8.34 29.87\n",
      "CorDA KPA 320M 46.21 10.64 7.33 21.39 45.03 6.54 25.79\n",
      "SC-LoRAβ= 0 320M 44.26 5.18 7.19 18.88 53.53 8.98 31.25\n",
      "β= 0.5 320M 48.91 7.70 6.89 21.17 53.37 8.62 31.00\n",
      "β= 0.8 320M 50.52 10.64 7.04 22.73 52.46 7.62 30.04\n",
      "Table 3: Results of world knowledge preservation and math ability after fine-tuning on MetaMATH.\n"
     ]
    }
   ],
   "source": [
    "print(context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0746a0-6927-49aa-945d-9fdf40357642",
   "metadata": {},
   "source": [
    "## Part 2: LangChain\n",
    "\n",
    "Now, let's see how we could use the [LangChain](https://www.langchain.com/) library to build our RAG system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c2c036ff-d069-4332-bfb3-db093d808b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "vector_store = FAISS.from_texts(chunks, embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "38289e2b-54b8-43f8-aaf1-b7dd4a7077a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    "    model_name=\"meta-llama/llama-4-scout:free\",\n",
    "    openai_api_key=api_key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "456941d9-6c0b-4f32-88c4-69c5579d9d57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'How does SC-LoRA differ from regular LoRA?',\n",
       " 'context': [Document(id='a0b7312c-7b6b-420f-882e-c210023329aa', metadata={}, page_content='methods, both in utility and safety metric. Com-\\npared to the original model, SC-LoRA ( β= 0.9)\\nexhibits almost no safety degradation, and achieves\\nbest utility, even surpassing full fine-tuning by 3.79\\npoints. When increasing the learning rate, LoRA\\nshows a sharp decline in safety alignment while\\nmath ability is increasing. LoRA (lr=2e-5) and\\nCorDA KPA, though preserving safety well, are\\ninsufficient in fine-tuning performance compared\\nto our method. PiSSA and CorDA IPA, though'),\n",
       "  Document(id='bcaef4f2-ab84-4573-89e1-83d7fd77bfae', metadata={}, page_content='sponses (score = 5) as harmfulness rate . Lower\\nvalues for both metrics indicate stronger safety of\\nthe model.\\n5Method #Params HS↓HR(%) ↓Utility ↑\\nLlama-2-7b-Chat - 1.100 1.212 24.13\\nFull fine-tuning 6738M 1.364 5.455 51.41\\nLoRA 320M 1.176 2.424 50.32\\nPiSSA 320M 1.252 4.242 51.87\\nCorDA IPA 320M 1.209 3.333 44.61\\nCorDA KPA 320M 1.106 0.606 50.89\\nSC-LoRAβ= 0.5 320M 1.161 1.818 52.54\\nβ= 0.7 320M 1.148 1.818 52.07\\nβ= 0.9 320M 1.097 0.000 51.67'),\n",
       "  Document(id='084da543-aaa2-4f9a-999c-22698b8fffcb', metadata={}, page_content='2019) with the following hyper-parameters: batch\\nsize 128, learning rate 2e-5 (except for experiment\\nin Section 4.2, where we tune the learning rate of\\nbaselines for better performance), cosine annealing\\nlearning rate schedule, warm-up ratio 0.03, and no\\nweight decay. The rank of LoRA and its variants\\nare all set to 128 for comparison. For SC-LoRA,\\nwe tune the hyperparameter βto find a good bal-\\nanced result. All experiment results are obtained\\nby running on only one seed.'),\n",
       "  Document(id='af81805b-72ae-40ea-abc3-27188da1b03c', metadata={}, page_content='Although SC-LoRA can successfully handle both\\nefficient fine-tuning and knowledge preservation at\\nthe same time, it still has drawbacks.\\nFirst, SC-LoRA is just a LoRA initialization\\nmethod, and does not strongly constrain the updates\\nduring fine-tuning process. Hence after fine-tuning\\non more complex tasks and with more steps, the\\nknowledge preservation ability can also drop (see\\nthe preservation drop of NQ-open in Table 3 for\\nexample).\\nSecond, its application on preserving other types')],\n",
       " 'answer': 'SC-LoRA is a LoRA initialization method that introduces a hyperparameter β to balance utility and safety. This allows SC-LoRA to achieve better safety and utility performance compared to regular LoRA. Unlike regular LoRA, SC-LoRA can preserve safety well while still achieving high utility.'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"How does SC-LoRA differ from regular LoRA?\"\n",
    "\n",
    "retriever = vector_store.as_retriever()\n",
    "\n",
    "system_prompt = (\n",
    "    \"Use the given context to answer the question. \"\n",
    "    \"If you don't know the answer, say you don't know. \"\n",
    "    \"Use three sentence maximum and keep the answer concise. \"\n",
    "    \"Context: {context}\"\n",
    ")\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "chain = create_retrieval_chain(retriever, question_answer_chain)\n",
    "\n",
    "chain.invoke({\"input\": query})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
